{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TRABALHO 03 - DETECÇÃO\n",
        "\n",
        "Alunos: Alani Rigotti, Ari Júnior, Felipe Araújo e Luigi Marchetti"
      ],
      "metadata": {
        "id": "8Lu9E6c9t9Cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliotecas\n",
        "\n",
        "**Ultralytics** é uma biblioteca voltada para detecção de objetos em vídeo e imagem. Foi utilizado os modelos yolo11n.pt, yolo11x.pt, yolo11m.pt e yolov8x.pt.\n",
        "\n",
        "`model = YOLO(\"yolo11n.pt\")`\n",
        "\n"
      ],
      "metadata": {
        "id": "o8GEBltNuVkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "WZVos0aSvNBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CV** é uma biblioteca que fornece as ferramentas para o processamento de imagens e análise de vídeo."
      ],
      "metadata": {
        "id": "4tJB_dl8vNg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vjEiBa3evSSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo escolhido\n",
        "\n",
        "A máquina utilizada possui um processador Ryzen 5 5600 com clock de 3.5 GHz e placa gráfica RTX 3060 com 3584 CUDA Cores\n",
        "\n",
        "| Modelo       | FPS | Ano  |\n",
        "|--------------|-----|------|\n",
        "| YOLO11n      | 20  | 2024 |\n",
        "| YOLO11m      | 5   | 2024 |\n",
        "| YOLO11x      | 2   | 2024 |\n",
        "| YOLOv8x      | 2   | 2023 |"
      ],
      "metadata": {
        "id": "ua8-VmIfwSI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the YOLO model\n",
        "model = YOLO(\"yolo11n.pt\")\n",
        "model = YOLO(\"yolov8x.pt\")\n",
        "model = YOLO(\"yolo11x.pt\")\n",
        "model = YOLO(\"yolo11m.pt\")"
      ],
      "metadata": {
        "id": "xHHC8ECCwRYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vídeo\n",
        "\n",
        "Os vídeos foram retirados do site **MOTChallenge**, ele disponibiliza um grande coleção de conjuntos de dados para a comunidade dev."
      ],
      "metadata": {
        "id": "R40QJyEPwYRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the video file\n",
        "video_path = \"test2.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "paused = False  # Flag de pausa"
      ],
      "metadata": {
        "id": "6tfMpYjbwXxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmo V1"
      ],
      "metadata": {
        "id": "uh9UkVS5yDY1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmUp60TyWuHx",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Loop principal: continua enquanto o vídeo estiver aberto\n",
        "while cap.isOpened():\n",
        "\n",
        "    # Só processa o próximo frame se não estiver pausado\n",
        "    if not paused:\n",
        "        # Lê o próximo frame do vídeo\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            break  # Sai do loop se não conseguir ler o frame (fim do vídeo ou erro)\n",
        "\n",
        "        # Obtém o timestamp atual em milissegundos\n",
        "        timestamp_ms = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
        "        total_ms = int(timestamp_ms)\n",
        "\n",
        "        # Converte o timestamp para o formato MM:SS.mmm\n",
        "        minutes = (total_ms // 1000) // 60\n",
        "        seconds = (total_ms // 1000) % 60\n",
        "        milliseconds = total_ms % 1000\n",
        "        timestamp_str = f\"{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n",
        "\n",
        "        # Obtém o número do frame atual\n",
        "        frame_id = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "\n",
        "        # Aplica o modelo YOLO para detecção de objetos no frame\n",
        "        results = model(frame, verbose=False)\n",
        "        boxes = results[0].boxes         # Caixas delimitadoras detectadas\n",
        "        names = model.names              # Nomes das classes reconhecidas pelo modelo\n",
        "        target_classes = [\"person\", \"car\"]  # Classes de interesse\n",
        "        count = {\"person\": 0, \"car\": 0}     # Contadores de objetos detectados\n",
        "\n",
        "        # Loop sobre cada caixa detectada\n",
        "        for box in boxes:\n",
        "            cls_id = int(box.cls[0])            # ID da classe detectada\n",
        "            class_name = names[cls_id]          # Nome da classe\n",
        "            if class_name in target_classes:    # Se for pessoa ou carro\n",
        "                count[class_name] += 1          # Incrementa contador\n",
        "\n",
        "                # Obtém coordenadas da caixa delimitadora\n",
        "                xyxy = box.xyxy[0].cpu().numpy().astype(int)\n",
        "                conf = float(box.conf[0])       # Confiança da detecção\n",
        "                label = f\"{class_name} {conf:.2f}\"\n",
        "\n",
        "                # Cor da caixa: verde para pessoa, azul para carro\n",
        "                color = (0, 255, 0) if class_name == \"person\" else (255, 0, 0)\n",
        "\n",
        "                # Desenha a caixa e o rótulo no frame\n",
        "                cv2.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), color, 2)\n",
        "                cv2.putText(frame, label, (xyxy[0], xyxy[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "        # Exibe no terminal o timestamp, número do frame e contagem de pessoas e carros detectados\n",
        "        print(f\"[{timestamp_str} | Frame {frame_id}] Pessoas: {count['person']}, Carros: {count['car']}\")\n",
        "\n",
        "    # Mostra o frame atual na janela (mesmo quando pausado)\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "    # Aguarda uma tecla ser pressionada por 1ms\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    if key == ord('q'):        # Tecla 'q' para sair\n",
        "        break\n",
        "    elif key == ord(' '):      # Tecla espaço para pausar/despausar\n",
        "        paused = not paused\n",
        "\n",
        "# Libera recursos ao final do vídeo\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algoritmo como parâmetros"
      ],
      "metadata": {
        "id": "uSS1I0hgzwGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parâmetros para contagem e verificação de duplicatas\n",
        "MIN_PERSIST_TIME_MS = 1000  # tempo mínimo para contar (em ms)\n",
        "DUPLICATE_DISTANCE_THRESHOLD = 50  # distância mínima para considerar duplicado (em pixels)\n",
        "DUPLICATE_TIME_THRESHOLD_MS = 1000  # tempo mínimo entre objetos próximos\n",
        "\n",
        "\n",
        "# Dicionários para armazenar histórico e IDs já contados\n",
        "track_history = {}\n",
        "counted_ids = {\"person\": set(), \"car\": set()}\n",
        "last_positions = {\"person\": [], \"car\": []}  # posições recentes para verificar duplicação\n",
        "\n",
        "# Função auxiliar para obter o centro da caixa delimitadora (bounding box)\n",
        "def get_center(xyxy):\n",
        "    x1, y1, x2, y2 = xyxy\n",
        "    return ((x1 + x2) // 2, (y1 + y2) // 2)\n",
        "\n",
        "# Loop principal para processar os frames do vídeo\n",
        "while cap.isOpened():\n",
        "    if not paused:\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            break  # Encerra se não houver mais frames\n",
        "\n",
        "        # Obtém timestamp e informações do frame atual\n",
        "        timestamp_ms = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
        "        minutes = (timestamp_ms // 1000) // 60\n",
        "        seconds = (timestamp_ms // 1000) % 60\n",
        "        milliseconds = timestamp_ms % 1000\n",
        "        timestamp_str = f\"{minutes:02d}:{seconds:02d}.{milliseconds:03d}\"\n",
        "        frame_id = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "\n",
        "        # Faz a inferência com o modelo YOLO com tracking\n",
        "        results = model.track(frame, persist=True, verbose=False)\n",
        "        boxes = results[0].boxes\n",
        "        names = model.names\n",
        "        target_classes = [\"person\", \"car\"]\n",
        "        current_count = {\"person\": 0, \"car\": 0}  # contagem atual no frame\n",
        "\n",
        "        # Se houver caixas com IDs (tracking ativo)\n",
        "        if boxes.id is not None:\n",
        "            for i in range(len(boxes.cls)):\n",
        "                cls_id = int(boxes.cls[i])\n",
        "                track_id = int(boxes.id[i])\n",
        "                class_name = names[cls_id]\n",
        "\n",
        "                # Pula classes que não queremos contar\n",
        "                if class_name not in target_classes:\n",
        "                    continue\n",
        "\n",
        "                # Obtém a posição e o centro da caixa\n",
        "                xyxy = boxes.xyxy[i].cpu().numpy().astype(int)\n",
        "                center = get_center(xyxy)\n",
        "\n",
        "                # Atualiza o histórico de rastreamento\n",
        "                if track_id not in track_history:\n",
        "                    track_history[track_id] = {\n",
        "                        \"class\": class_name,\n",
        "                        \"first_seen\": timestamp_ms,\n",
        "                        \"last_seen\": timestamp_ms,\n",
        "                        \"center\": center,\n",
        "                        \"counted\": False,\n",
        "                    }\n",
        "                else:\n",
        "                    track_history[track_id][\"last_seen\"] = timestamp_ms\n",
        "                    track_history[track_id][\"center\"] = center\n",
        "\n",
        "                # Verifica se o objeto deve ser contado\n",
        "                track = track_history[track_id]\n",
        "                duration = track[\"last_seen\"] - track[\"first_seen\"]\n",
        "\n",
        "                if not track[\"counted\"] and duration >= MIN_PERSIST_TIME_MS:\n",
        "                    # Verifica se é uma duplicata próxima\n",
        "                    too_close = False\n",
        "                    for prev_id, prev_center, prev_time in last_positions[class_name]:\n",
        "                        dist = np.linalg.norm(np.array(center) - np.array(prev_center))\n",
        "                        time_diff = timestamp_ms - prev_time\n",
        "                        if dist < DUPLICATE_DISTANCE_THRESHOLD and time_diff < DUPLICATE_TIME_THRESHOLD_MS:\n",
        "                            too_close = True\n",
        "                            break\n",
        "\n",
        "                    # Se não for duplicado, conta o objeto\n",
        "                    if not too_close:\n",
        "                        counted_ids[class_name].add(track_id)\n",
        "                        last_positions[class_name].append((track_id, center, timestamp_ms))\n",
        "                        track[\"counted\"] = True\n",
        "\n",
        "                # Se o objeto já foi contado, desenha no frame e atualiza contagem\n",
        "                if track[\"counted\"]:\n",
        "                    current_count[class_name] += 1\n",
        "                    label = f\"{class_name} ID:{track_id}\"\n",
        "                    color = (0, 255, 0) if class_name == \"person\" else (255, 0, 0)\n",
        "                    cv2.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), color, 2)\n",
        "                    cv2.putText(frame, label, (xyxy[0], xyxy[1] - 10),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "        # Total de pessoas e carros únicos já contados\n",
        "        total_persons = len(counted_ids[\"person\"])\n",
        "        total_cars = len(counted_ids[\"car\"])\n",
        "\n",
        "        # Exibe as informações no terminal\n",
        "        print(f\"[{timestamp_str} | Frame {frame_id}] Pessoas ativas: {current_count['person']} (total únicas: {total_persons}), Carros ativos: {current_count['car']} (total únicos: {total_cars})\")\n",
        "\n",
        "    # Mostra o frame com as detecções\n",
        "    cv2.imshow(\"YOLO + Tracking + Contagem Robusta\", frame)\n",
        "\n",
        "    # Comandos do teclado: 'q' para sair, espaço para pausar/resumir\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    if key == ord('q'):\n",
        "        break\n",
        "    elif key == ord(' '):\n",
        "        paused = not paused\n",
        "\n",
        "# Libera os recursos ao final\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "8szcK1vQzdRz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}